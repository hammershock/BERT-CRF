{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 资源\n",
    "\n",
    " ### **⭐ ⭐ ⭐ 欢迎点个小小的[Star](https://github.com/PaddlePaddle/awesome-DeepLearning/stargazers)支持！⭐ ⭐ ⭐** \n",
    "  \n",
    "开源不易，希望大家多多支持~ \n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/c0fc093bffd84dc8920b33e8bf445bb0e842bc9fc29047878df03eb84691f0bf' width='700'></center>\n",
    "\n",
    "* 更多CV和NLP中的transformer模型(BERT、ERNIE、ViT、DeiT、Swin Transformer等)、深度学习资料，请参考：[awesome-DeepLearning](https://github.com/paddlepaddle/awesome-DeepLearning)\n",
    "\n",
    "* 更多的预训练语言模型，请参考[paddleNLP](https://github.com/PaddlePaddle/PaddleNLP): https://github.com/PaddlePaddle/PaddleNLP\n",
    "\n",
    "* 飞桨框架相关资料，请参考：[飞桨深度学习平台](https://www.paddlepaddle.org.cn/?fr=paddleEdu_aistudio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、竞赛介绍\n",
    "\n",
    " CCF大数据与计算智能大赛（CCF Big Data & Computing Intelligence Contest，简称CCF BDCI）由中国计算机学会于2013年创办。大赛由国家自然科学基金委员会指导，是大数据与人工智能领域的算法、应用和系统大型挑战赛事。大赛面向重点行业和应用领域征集需求，以前沿技术与行业应用问题为导向，以促进行业发展及产业升级为目标，以众智、众包的方式，汇聚海内外产学研用多方智慧，为社会发现和培养了大量高质量数据人才。\n",
    "  大赛迄今已成功举办八届，累计吸引全球1500余所高校、1800家企事业单位及80余所科研机构的12万余人参与，已成为中国大数据与人工智能领域最具影响力的活动之一，是中国大数据综合赛事第一品牌。\n",
    "  2021年第九届大赛以“数引创新，竞促汇智”为主题，立足余杭、面向全球，于9月至12月举办。大赛将致力于解决来自政府、企业真实场景中的痛点、难点问题，邀请全球优秀团队参与数据资源开发利用，广泛征集信息技术应用解决方案。\n",
    "  \n",
    "## 1.1  赛题任务\n",
    "\n",
    "比赛的地址为[https://www.datafountain.cn/competitions/529](https://www.datafountain.cn/competitions/529)\n",
    "\n",
    "观点提取旨在从非结构化的评论文本中提取标准化、结构化的信息，如产品名、评论维度、评论观点等。此处希望大家能够通过自然语言处理的语义情感分析技术判断出一段银行产品评论文本的情感倾向，并能进一步通过语义分析和实体识别，标识出评论所讨论的产品名，评价指标和评价关键词。\n",
    "\n",
    "\n",
    "因此我们就可以分为命名实体识别和情感分类两个任务来做，然后把这两个任务的结果合并提交到官网就行了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、命名实体识别部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:26:45.130296Z",
     "iopub.status.busy": "2024-06-05T14:26:45.129634Z",
     "iopub.status.idle": "2024-06-05T14:26:45.616498Z",
     "shell.execute_reply": "2024-06-05T14:26:45.615474Z",
     "shell.execute_reply.started": "2024-06-05T14:26:45.130254Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import inspect\n",
    "import numpy as np\n",
    "import collections\n",
    "import pandas as pd\n",
    "from tqdm import tqdm \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-05T14:26:47.322886Z",
     "iopub.status.busy": "2024-06-05T14:26:47.322269Z",
     "iopub.status.idle": "2024-06-05T14:26:47.864394Z",
     "shell.execute_reply": "2024-06-05T14:26:47.863229Z",
     "shell.execute_reply.started": "2024-06-05T14:26:47.322844Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  data/data114938/产品评论观点提取-new.zip\r\n",
      "  inflating: data/submit_example.csv  \r\n",
      "  inflating: data/__MACOSX/._submit_example.csv  \r\n",
      "  inflating: data/test_public.csv    \r\n",
      "  inflating: data/__MACOSX/._test_public.csv  \r\n",
      "  inflating: data/train_data_public.csv  \r\n",
      "  inflating: data/__MACOSX/._train_data_public.csv  \r\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# !unzip -o data/data110473/产品评论观点提取.zip -d data\n",
    "!unzip -o data/data114938/产品评论观点提取-new.zip -d data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-06-05T14:26:53.788301Z",
     "iopub.status.busy": "2024-06-05T14:26:53.787701Z",
     "iopub.status.idle": "2024-06-05T14:26:53.836010Z",
     "shell.execute_reply": "2024-06-05T14:26:53.835039Z",
     "shell.execute_reply.started": "2024-06-05T14:26:53.788268Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...</td>\n",
       "      <td>B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>单标我有了，最近visa双标返现活动好</td>\n",
       "      <td>B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>建设银行提额很慢的……</td>\n",
       "      <td>B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k</td>\n",
       "      <td>O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>利率不错，可以撸</td>\n",
       "      <td>B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  \\\n",
       "id                                                      \n",
       "0   交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第...   \n",
       "1                                 单标我有了，最近visa双标返现活动好   \n",
       "2                                         建设银行提额很慢的……   \n",
       "3                  我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k   \n",
       "4                                            利率不错，可以撸   \n",
       "\n",
       "                                             BIO_anno  class  \n",
       "id                                                            \n",
       "0   B-BANK I-BANK O O O O O O O O O O B-COMMENTS_N...      0  \n",
       "1   B-PRODUCT I-PRODUCT O O O O O O B-PRODUCT I-PR...      1  \n",
       "2   B-BANK I-BANK I-BANK I-BANK B-COMMENTS_N I-COM...      0  \n",
       "3   O O O O O O O O O O B-COMMENTS_N I-COMMENTS_N ...      2  \n",
       "4   B-COMMENTS_N I-COMMENTS_N B-COMMENTS_ADJ I-COM...      1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name='data/train_data_public.csv'\n",
    "data=pd.read_csv(file_name,index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:26:55.627565Z",
     "iopub.status.busy": "2024-06-05T14:26:55.627005Z",
     "iopub.status.idle": "2024-06-05T14:26:55.632758Z",
     "shell.execute_reply": "2024-06-05T14:26:55.632030Z",
     "shell.execute_reply.started": "2024-06-05T14:26:55.627517Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['text', 'BIO_anno', 'class'], dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:26:57.148071Z",
     "iopub.status.busy": "2024-06-05T14:26:57.147475Z",
     "iopub.status.idle": "2024-06-05T14:26:59.274635Z",
     "shell.execute_reply": "2024-06-05T14:26:59.273135Z",
     "shell.execute_reply.started": "2024-06-05T14:26:57.148032Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 导入paddle库\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "import paddle.nn as nn\n",
    "from paddle.io import DataLoader\n",
    "from paddle.dataset.common import md5file\n",
    "# 导入paddlenlp的库\n",
    "import paddlenlp as ppnlp\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "\n",
    "from paddlenlp.data import Stack, Tuple, Pad, Dict\n",
    "from paddlenlp.datasets import DatasetBuilder,get_path_from_url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:01.355344Z",
     "iopub.status.busy": "2024-06-05T14:27:01.354691Z",
     "iopub.status.idle": "2024-06-05T14:27:01.363081Z",
     "shell.execute_reply": "2024-06-05T14:27:01.362281Z",
     "shell.execute_reply.started": "2024-06-05T14:27:01.355306Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class ProductCommentDataset(DatasetBuilder):\n",
    "\n",
    "    SPLITS = {\n",
    "        'train': os.path.join('data','train_data_public.csv'),\n",
    "        'test': os.path.join('data', 'test_public.csv'),\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        default_root = '.'\n",
    "        filename = self.SPLITS[mode]\n",
    "        fullname = os.path.join(default_root, filename)\n",
    "        self.mode=mode\n",
    "        return fullname\n",
    "\n",
    "    def _read(self, filename, *args):\n",
    "        df=pd.read_csv(filename)\n",
    "        for idx,row in df.iterrows():\n",
    "            text=row['text']\n",
    "            if(type(text)==float):\n",
    "                print(text)\n",
    "                continue\n",
    "            tokens=list(row['text'])\n",
    "            if(self.mode=='test'):\n",
    "                tags=[]\n",
    "            else:\n",
    "                tags=row['BIO_anno'].split()\n",
    "\n",
    "            yield {\"tokens\": tokens, \"labels\": tags}\n",
    "\n",
    "    def get_labels(self):\n",
    "\n",
    "        return [\"B-BANK\", \"I-BANK\", \"B-PRODUCT\", \"I-PRODUCT\",'B-COMMENTS_N','I-COMMENTS_N','B-COMMENTS_ADJ','I-COMMENTS_ADJ','O']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:03.377915Z",
     "iopub.status.busy": "2024-06-05T14:27:03.377360Z",
     "iopub.status.idle": "2024-06-05T14:27:03.382628Z",
     "shell.execute_reply": "2024-06-05T14:27:03.381889Z",
     "shell.execute_reply.started": "2024-06-05T14:27:03.377878Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(path_or_read_func,\n",
    "                 name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "    reader_instance = ProductCommentDataset(lazy=lazy, name=name, **kwargs)\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:04.906229Z",
     "iopub.status.busy": "2024-06-05T14:27:04.905613Z",
     "iopub.status.idle": "2024-06-05T14:27:05.850054Z",
     "shell.execute_reply": "2024-06-05T14:27:05.848990Z",
     "shell.execute_reply.started": "2024-06-05T14:27:04.906188Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create dataset, tokenizer and dataloader.\n",
    "train_ds, test_ds = load_dataset('ProductCommentDataset', splits=('train', 'test'), lazy=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:07.504305Z",
     "iopub.status.busy": "2024-06-05T14:27:07.503719Z",
     "iopub.status.idle": "2024-06-05T14:27:07.510380Z",
     "shell.execute_reply": "2024-06-05T14:27:07.509515Z",
     "shell.execute_reply.started": "2024-06-05T14:27:07.504255Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def tokenize_and_align_labels(example, tokenizer, no_entity_id,\n",
    "                              max_seq_len=512):\n",
    "    labels = example['labels']\n",
    "    example = example['tokens']\n",
    "    tokenized_input = tokenizer(\n",
    "        example,\n",
    "        return_length=True,\n",
    "        is_split_into_words=True,\n",
    "        max_seq_len=max_seq_len)\n",
    "\n",
    "    # -2 for [CLS] and [SEP]\n",
    "    if len(tokenized_input['input_ids']) - 2 < len(labels):\n",
    "        labels = labels[:len(tokenized_input['input_ids']) - 2]\n",
    "    tokenized_input['labels'] = [no_entity_id] + labels + [no_entity_id]\n",
    "    tokenized_input['labels'] += [no_entity_id] * (\n",
    "        len(tokenized_input['input_ids']) - len(tokenized_input['labels']))\n",
    "    return tokenized_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:09.157012Z",
     "iopub.status.busy": "2024-06-05T14:27:09.156403Z",
     "iopub.status.idle": "2024-06-05T14:27:22.050033Z",
     "shell.execute_reply": "2024-06-05T14:27:22.048945Z",
     "shell.execute_reply.started": "2024-06-05T14:27:09.156976Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-05 22:27:09,161] [    INFO] - Downloading bert-base-chinese-vocab.txt from https://paddle-hapi.bj.bcebos.com/models/bert/bert-base-chinese-vocab.txt\r\n",
      "100%|██████████| 107/107 [00:00<00:00, 4048.34it/s]\r\n",
      "[2024-06-05 22:27:09,338] [    INFO] - Downloading http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams and saved to /home/aistudio/.paddlenlp/models/bert-base-chinese\r\n",
      "[2024-06-05 22:27:09,341] [    INFO] - Downloading bert-base-chinese.pdparams from http://paddlenlp.bj.bcebos.com/models/transformers/bert/bert-base-chinese.pdparams\r\n",
      "100%|██████████| 465291/465291 [00:06<00:00, 72532.76it/s]\r\n",
      "W0605 22:27:15.883476   362 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 12.0, Runtime API Version: 10.1\r\n",
      "W0605 22:27:15.930539   362 device_context.cc:465] device: 0, cuDNN Version: 7.6.\r\n"
     ]
    }
   ],
   "source": [
    "max_seq_length=128\n",
    "batch_size=64\n",
    "label_list = train_ds.label_list\n",
    "label_num = len(label_list)\n",
    "no_entity_id = label_num - 1\n",
    "# num_train_epochs=3\n",
    "\n",
    "from paddlenlp.transformers import BertTokenizer,BertPretrainedModel,BertForTokenClassification\n",
    "from paddlenlp.transformers import ErnieModel,ErnieForTokenClassification,ErnieTokenizer\n",
    "\n",
    "# model_name_or_path='macbert-base-chinese'\n",
    "model_name_or_path='bert-base-chinese'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n",
    "# Define the model netword and its loss\n",
    "# last_step = num_train_epochs * len(train_data_loader)\n",
    "model = BertForTokenClassification.from_pretrained(model_name_or_path, num_classes=label_num)\n",
    "paddle.device.cuda.empty_cache()\n",
    "# model_name_or_path='ernie-1.0'\n",
    "# tokenizer = ErnieTokenizer.from_pretrained(model_name_or_path)\n",
    "# model = ErnieForTokenClassification.from_pretrained(model_name_or_path, num_classes=label_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:22.068336Z",
     "iopub.status.busy": "2024-06-05T14:27:22.067863Z",
     "iopub.status.idle": "2024-06-05T14:27:22.075592Z",
     "shell.execute_reply": "2024-06-05T14:27:22.074893Z",
     "shell.execute_reply.started": "2024-06-05T14:27:22.068312Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "trans_func = partial(\n",
    "        tokenize_and_align_labels,\n",
    "        tokenizer=tokenizer,\n",
    "        no_entity_id=no_entity_id,\n",
    "        max_seq_len=max_seq_length)\n",
    "        \n",
    "train_ds = train_ds.map(trans_func)\n",
    "\n",
    "ignore_label = -100\n",
    "\n",
    "batchify_fn = lambda samples, fn=Dict({\n",
    "        'input_ids': Pad(axis=0, pad_val=tokenizer.pad_token_id, dtype='int32'),  # input\n",
    "        'token_type_ids': Pad(axis=0, pad_val=tokenizer.pad_token_type_id, dtype='int32'),  # segment\n",
    "        'seq_len': Stack(dtype='int64'),  # seq_len\n",
    "        'labels': Pad(axis=0, pad_val=no_entity_id, dtype='int64')  # label\n",
    "    }): fn(samples)\n",
    "\n",
    "\n",
    "train_batch_sampler = paddle.io.DistributedBatchSampler(train_ds, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "        dataset=train_ds,\n",
    "        collate_fn=batchify_fn,\n",
    "        num_workers=0,\n",
    "        batch_sampler=train_batch_sampler,\n",
    "        return_list=True)\n",
    "\n",
    "test_ds = test_ds.map(trans_func)\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "        dataset=test_ds,\n",
    "        collate_fn=batchify_fn,\n",
    "        num_workers=0,\n",
    "        batch_size=batch_size,\n",
    "        return_list=True)\n",
    "paddle.device.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:26.323609Z",
     "iopub.status.busy": "2024-06-05T14:27:26.322961Z",
     "iopub.status.idle": "2024-06-05T14:27:26.330828Z",
     "shell.execute_reply": "2024-06-05T14:27:26.329968Z",
     "shell.execute_reply.started": "2024-06-05T14:27:26.323570Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class BertForTokenClassification(BertPretrainedModel):\n",
    "\n",
    "    def __init__(self, bert, num_classes=2, dropout=None):\n",
    "        super(BertForTokenClassification, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.bert = bert  # allow bert to be config\n",
    "        self.dropout = nn.Dropout(dropout if dropout is not None else\n",
    "                                  self.bert.config[\"hidden_dropout_prob\"])\n",
    "        self.classifier = nn.Linear(self.bert.config[\"hidden_size\"],\n",
    "                                    num_classes)\n",
    "        self.apply(self.init_weights)\n",
    "\n",
    "    def forward(self,\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                position_ids=None,\n",
    "                attention_mask=None):\n",
    "        sequence_output, _ = self.bert(\n",
    "            input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            attention_mask=attention_mask)\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:31.114967Z",
     "iopub.status.busy": "2024-06-05T14:27:31.114257Z",
     "iopub.status.idle": "2024-06-05T14:27:31.126198Z",
     "shell.execute_reply": "2024-06-05T14:27:31.125094Z",
     "shell.execute_reply.started": "2024-06-05T14:27:31.114917Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_train_epochs=3\n",
    "warmup_steps=0\n",
    "\n",
    "max_steps=-1\n",
    "learning_rate=5e-5\n",
    "adam_epsilon=1e-8\n",
    "weight_decay=0.0\n",
    "device='gpu'\n",
    "paddle.set_device(device)\n",
    "\n",
    "\n",
    "logging_steps=200\n",
    "\n",
    "\n",
    "save_steps=100\n",
    "output_dir='checkpoint'\n",
    "os.makedirs(output_dir,exist_ok=True)\n",
    "\n",
    "\n",
    "num_training_steps = max_steps if max_steps > 0 else len(\n",
    "        train_data_loader) * num_train_epochs\n",
    "\n",
    "last_step = num_train_epochs * len(train_data_loader)\n",
    "\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate, num_training_steps,\n",
    "                                         warmup_steps)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "        p.name for n, p in model.named_parameters()\n",
    "        if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "    ]\n",
    "\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "        learning_rate=lr_scheduler,\n",
    "        epsilon=adam_epsilon,\n",
    "        parameters=model.parameters(),\n",
    "        weight_decay=weight_decay,\n",
    "        apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "loss_fct = nn.loss.CrossEntropyLoss(ignore_index=ignore_label)\n",
    "\n",
    "metric = ChunkEvaluator(label_list=label_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:35.742621Z",
     "iopub.status.busy": "2024-06-05T14:27:35.742005Z",
     "iopub.status.idle": "2024-06-05T14:27:35.749515Z",
     "shell.execute_reply": "2024-06-05T14:27:35.748638Z",
     "shell.execute_reply.started": "2024-06-05T14:27:35.742581Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loss_fct, metric, data_loader, label_num):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    avg_loss, precision, recall, f1_score = 0, 0, 0, 0\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids, length, labels = batch\n",
    "        logits = model(input_ids, token_type_ids)\n",
    "        loss = loss_fct(logits, labels)\n",
    "        avg_loss = paddle.mean(loss)\n",
    "        preds = logits.argmax(axis=2)\n",
    "        num_infer_chunks, num_label_chunks, num_correct_chunks = metric.compute(\n",
    "            length, preds, labels)\n",
    "        metric.update(num_infer_chunks.numpy(),\n",
    "                      num_label_chunks.numpy(), num_correct_chunks.numpy())\n",
    "        precision, recall, f1_score = metric.accumulate()\n",
    "    print(\"eval loss: %f, precision: %f, recall: %f, f1: %f\" %\n",
    "          (avg_loss, precision, recall, f1_score))\n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:38.104605Z",
     "iopub.status.busy": "2024-06-05T14:27:38.103982Z",
     "iopub.status.idle": "2024-06-05T14:27:38.111931Z",
     "shell.execute_reply": "2024-06-05T14:27:38.111037Z",
     "shell.execute_reply.started": "2024-06-05T14:27:38.104565Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def do_train(model,train_data_loader):\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "    for epoch in range(num_train_epochs):\n",
    "        for step, batch in enumerate(train_data_loader):\n",
    "            global_step += 1\n",
    "            input_ids, token_type_ids, _, labels = batch\n",
    "            logits = model(input_ids, token_type_ids)\n",
    "            loss = loss_fct(logits, labels)\n",
    "            avg_loss = paddle.mean(loss)\n",
    "            if global_step % logging_steps == 0:\n",
    "                print(\"global step %d, epoch: %d, batch: %d, loss: %f, speed: %.2f step/s\"\n",
    "                        % (global_step, epoch, step, avg_loss,\n",
    "                        logging_steps / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            avg_loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "        if global_step % save_steps == 0 or global_step == last_step:\n",
    "            model_path=os.path.join(output_dir,\"model_ner_%d.pdparams\" % global_step)\n",
    "            paddle.save(model.state_dict(),model_path)\n",
    "\n",
    "#do_train(model,train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:40.425648Z",
     "iopub.status.busy": "2024-06-05T14:27:40.424926Z",
     "iopub.status.idle": "2024-06-05T14:27:40.431617Z",
     "shell.execute_reply": "2024-06-05T14:27:40.430776Z",
     "shell.execute_reply.started": "2024-06-05T14:27:40.425593Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def parse_decodes(input_words, id2label, decodes, lens):\n",
    "    decodes = [x for batch in decodes for x in batch]\n",
    "    lens = [x for batch in lens for x in batch]\n",
    "\n",
    "    outputs = []\n",
    "    for idx, end in enumerate(lens):\n",
    "        sent = \"\".join(input_words[idx]['tokens'])\n",
    "        tags = [id2label[x] for x in decodes[idx][1:end-1]]\n",
    "        outputs.append([sent,tags])\n",
    "       \n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:27:42.759275Z",
     "iopub.status.busy": "2024-06-05T14:27:42.758582Z",
     "iopub.status.idle": "2024-06-05T14:27:48.709200Z",
     "shell.execute_reply": "2024-06-05T14:27:48.708167Z",
     "shell.execute_reply.started": "2024-06-05T14:27:42.759235Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\r\n"
     ]
    }
   ],
   "source": [
    "state_dict=paddle.load('checkpoint/model_ner_351.pdparams')\n",
    "print('loading model')\n",
    "model.load_dict(state_dict)\n",
    "\n",
    "id2label = dict(enumerate(test_ds.label_list))\n",
    "raw_data = test_ds.data\n",
    "\n",
    "model.eval()\n",
    "pred_list = []\n",
    "len_list = []\n",
    "for step, batch in enumerate(test_data_loader):\n",
    "    input_ids, token_type_ids, length, labels = batch\n",
    "    logits = model(input_ids, token_type_ids)\n",
    "    pred = paddle.argmax(logits, axis=-1)\n",
    "    pred_list.append(pred.numpy())\n",
    "    len_list.append(length.numpy())\n",
    "preds = parse_decodes(raw_data, id2label, pred_list, len_list)\n",
    "paddle.device.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:06.652509Z",
     "iopub.status.busy": "2024-06-05T14:28:06.651885Z",
     "iopub.status.idle": "2024-06-05T14:28:06.658330Z",
     "shell.execute_reply": "2024-06-05T14:28:06.657467Z",
     "shell.execute_reply.started": "2024-06-05T14:28:06.652469Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['共享一个额度，没啥必要，四个卡不要年费吗？你这种人头，银行最喜欢，广发是出了名的风控严，套现就给你封...', ['O', 'O', 'O', 'O', 'B-COMMENTS_N', 'I-COMMENTS_N', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COMMENTS_N', 'I-COMMENTS_N', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-COMMENTS_ADJ', 'B-COMMENTS_ADJ', 'I-COMMENTS_ADJ', 'O', 'B-BANK', 'I-BANK', 'O', 'O', 'O', 'O', 'O', 'B-COMMENTS_N', 'I-COMMENTS_N', 'B-COMMENTS_ADJ', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']], ['炸了，就2000.浦发没那么好心，草', ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-BANK', 'I-BANK', 'O', 'B-COMMENTS_ADJ', 'O', 'B-COMMENTS_ADJ', 'I-COMMENTS_ADJ', 'O', 'O']], ['挂了电话自己打过去分期提额可以少分一点的', ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'B-COMMENTS_N', 'I-COMMENTS_N', 'O', 'O', 'O', 'O', 'O', 'O', 'O']], ['比如你首卡10k，二卡也10k，信报上显示邮政总共给你的授信额度是20k', ['O', 'O', 'O', 'B-PRODUCT', 'I-PRODUCT', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-BANK', 'I-BANK', 'O', 'O', 'O', 'O', 'O', 'B-COM\r\n"
     ]
    }
   ],
   "source": [
    "print(preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:08.809152Z",
     "iopub.status.busy": "2024-06-05T14:28:08.808505Z",
     "iopub.status.idle": "2024-06-05T14:28:08.815778Z",
     "shell.execute_reply": "2024-06-05T14:28:08.814971Z",
     "shell.execute_reply.started": "2024-06-05T14:28:08.809112Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bio_label=[' '.join(item[1]) for item in preds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、文本分类部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:11.241855Z",
     "iopub.status.busy": "2024-06-05T14:28:11.241093Z",
     "iopub.status.idle": "2024-06-05T14:28:11.247229Z",
     "shell.execute_reply": "2024-06-05T14:28:11.246247Z",
     "shell.execute_reply.started": "2024-06-05T14:28:11.241797Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.transformers import BertForSequenceClassification, BertTokenizer\n",
    "from paddlenlp.transformers import LinearDecayWithWarmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:13.502872Z",
     "iopub.status.busy": "2024-06-05T14:28:13.502213Z",
     "iopub.status.idle": "2024-06-05T14:28:14.061249Z",
     "shell.execute_reply": "2024-06-05T14:28:14.060206Z",
     "shell.execute_reply.started": "2024-06-05T14:28:13.502829Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read(data_path):\n",
    "    df=pd.read_csv(data_path)\n",
    "    for idx,row in df.iterrows():\n",
    "        words=row['text']\n",
    "        labels=row['class']\n",
    "        yield {'text': words, 'label': labels}\n",
    "\n",
    "# data_path为read()方法的参数\n",
    "train_ds = load_dataset(read, data_path='data/train_data_public.csv',lazy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:16.176908Z",
     "iopub.status.busy": "2024-06-05T14:28:16.176319Z",
     "iopub.status.idle": "2024-06-05T14:28:16.183025Z",
     "shell.execute_reply": "2024-06-05T14:28:16.182150Z",
     "shell.execute_reply.started": "2024-06-05T14:28:16.176871Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '交行14年用过，半年准备提额，却直接被降到1Ｋ，半年期间只T过一次三千，其它全部真实消费，第六个月的时候为了增加评分提额，还特意分期两万，但降额后电话投诉，申请提...', 'label': 0}, {'text': '单标我有了，最近visa双标返现活动好', 'label': 1}, {'text': '建设银行提额很慢的……', 'label': 0}, {'text': '我的怎么显示0.25费率，而且不管分多少期都一样费率，可惜只有69k', 'label': 2}]\r\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:18.173397Z",
     "iopub.status.busy": "2024-06-05T14:28:18.172805Z",
     "iopub.status.idle": "2024-06-05T14:28:18.210685Z",
     "shell.execute_reply": "2024-06-05T14:28:18.209583Z",
     "shell.execute_reply.started": "2024-06-05T14:28:18.173361Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-05 22:28:18,177] [    INFO] - Found /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese-vocab.txt\r\n"
     ]
    }
   ],
   "source": [
    "# 转换成id的函数\n",
    "def convert_example(example, tokenizer):\n",
    "    encoded_inputs = tokenizer(text=example[\"text\"], max_seq_len=512, pad_to_max_seq_len=True)\n",
    "    return tuple([np.array(x, dtype=\"int64\") for x in [\n",
    "            encoded_inputs[\"input_ids\"], encoded_inputs[\"token_type_ids\"], [example[\"label\"]]]])\n",
    "# 加载BERT的分词器\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "# 把训练集合转换成id\n",
    "train_ds = train_ds.map(partial(convert_example, tokenizer=tokenizer))\n",
    "\n",
    "# 构建训练集合的dataloader\n",
    "train_batch_sampler = paddle.io.BatchSampler(dataset=train_ds, batch_size=32, shuffle=True)\n",
    "train_data_loader = paddle.io.DataLoader(dataset=train_ds, batch_sampler=train_batch_sampler, return_list=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:19.919993Z",
     "iopub.status.busy": "2024-06-05T14:28:19.919305Z",
     "iopub.status.idle": "2024-06-05T14:28:22.070890Z",
     "shell.execute_reply": "2024-06-05T14:28:22.069800Z",
     "shell.execute_reply.started": "2024-06-05T14:28:19.919949Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-05 22:28:19,921] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/bert-base-chinese/bert-base-chinese.pdparams\r\n"
     ]
    }
   ],
   "source": [
    "num_classes=3\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:22.088659Z",
     "iopub.status.busy": "2024-06-05T14:28:22.088194Z",
     "iopub.status.idle": "2024-06-05T14:28:22.094598Z",
     "shell.execute_reply": "2024-06-05T14:28:22.093890Z",
     "shell.execute_reply.started": "2024-06-05T14:28:22.088635Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Layer):\n",
    "    def __init__(self, alpha=0.5, gamma=2, weight=None, ignore_index=255):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        # 参数可调\n",
    "        self.weight = paddle.to_tensor(np.array([1.063, 4.468, 1.021]))\n",
    "        self.ignore_index = ignore_index\n",
    "        self.ce_fn = nn.CrossEntropyLoss(weight=self.weight, soft_label=False) \n",
    " \n",
    "    def forward(self, preds, labels):\n",
    "        logpt = -self.ce_fn(preds, labels)\n",
    "        pt = paddle.exp(logpt)\n",
    "        loss = -((1 - pt) ** self.gamma) * self.alpha * logpt\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:35.663587Z",
     "iopub.status.busy": "2024-06-05T14:28:35.662971Z",
     "iopub.status.idle": "2024-06-05T14:28:35.674622Z",
     "shell.execute_reply": "2024-06-05T14:28:35.673774Z",
     "shell.execute_reply.started": "2024-06-05T14:28:35.663550Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paddle.device.cuda.empty_cache()\n",
    "num_train_epochs=3\n",
    "num_training_steps = len(train_data_loader) * num_train_epochs\n",
    "\n",
    "# 定义 learning_rate_scheduler，负责在训练过程中对 lr 进行调度\n",
    "lr_scheduler = LinearDecayWithWarmup(5E-5, num_training_steps, 0.0)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "\n",
    "# 定义 Optimizer\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=0.0,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "# 交叉熵损失和Focal 损失，两者可以切换\n",
    "# criterion = paddle.nn.loss.CrossEntropyLoss()\n",
    "criterion=FocalLoss()\n",
    "# 评估的时候采用准确率指标\n",
    "metric = paddle.metric.Accuracy()\n",
    "paddle.device.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:49.240821Z",
     "iopub.status.busy": "2024-06-05T14:28:49.240240Z",
     "iopub.status.idle": "2024-06-05T14:28:49.249901Z",
     "shell.execute_reply": "2024-06-05T14:28:49.248943Z",
     "shell.execute_reply.started": "2024-06-05T14:28:49.240786Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "paddle.device.cuda.empty_cache()\n",
    "# 接下来，开始正式训练模型，训练时间较长，可注释掉这部分\n",
    "def do_train(model,train_data_loader):\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "\n",
    "    for epoch in range(1, num_train_epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "            probs=paddle.to_tensor(probs, dtype=\"float64\")\n",
    "            loss = criterion(probs, labels)\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            # 每间隔 100 step 输出训练指标\n",
    "            if global_step % 100 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                        10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if global_step % save_steps == 0 or global_step == last_step:\n",
    "                model_path=os.path.join(output_dir,\"model_classfication_%d.pdparams\" % global_step)\n",
    "                paddle.save(model.state_dict(),model_path)\n",
    "# 正常训练\n",
    "#do_train(model,train_data_loader)\n",
    "paddle.device.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:28:52.797021Z",
     "iopub.status.busy": "2024-06-05T14:28:52.796423Z",
     "iopub.status.idle": "2024-06-05T14:28:52.805412Z",
     "shell.execute_reply": "2024-06-05T14:28:52.804486Z",
     "shell.execute_reply.started": "2024-06-05T14:28:52.796977Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    \"\"\"针对embedding层梯度上升干扰的对抗训练方法,Fast Gradient Method（FGM）\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and emb_name in name:  # 检验参数是否可训练及范围\n",
    "                self.backup[name] = param.numpy()  # 备份原有参数值\n",
    "                grad_tensor = paddle.to_tensor(param.grad)  # param.grad是个numpy对象\n",
    "                norm = paddle.norm(grad_tensor)  # norm化\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * grad_tensor / norm\n",
    "                    param.add(r_at)  # 在原有embed值上添加向上梯度干扰\n",
    "\n",
    "    def restore(self, emb_name='emb'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if not param.stop_gradient and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.set_value(self.backup[name])  # 将原有embed参数还原\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:29:24.306109Z",
     "iopub.status.busy": "2024-06-05T14:29:24.305007Z",
     "iopub.status.idle": "2024-06-05T14:29:24.309461Z",
     "shell.execute_reply": "2024-06-05T14:29:24.308711Z",
     "shell.execute_reply.started": "2024-06-05T14:29:24.306062Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(model.named_parameters())\n",
    "#for k,v in model.named_parameters():\n",
    "#   print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:30:01.623008Z",
     "iopub.status.busy": "2024-06-05T14:30:01.622357Z",
     "iopub.status.idle": "2024-06-05T14:30:01.632649Z",
     "shell.execute_reply": "2024-06-05T14:30:01.631659Z",
     "shell.execute_reply.started": "2024-06-05T14:30:01.622966Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 接下来，开始正式训练模型，训练时间较长，可注释掉这部分\n",
    "paddle.device.cuda.empty_cache()\n",
    "def do_adversarial_train(model,train_data_loader):\n",
    "\n",
    "    fgm = FGM(model)\n",
    "    global_step = 0\n",
    "    tic_train = time.time()\n",
    "\n",
    "    for epoch in range(1, num_train_epochs + 1):\n",
    "        for step, batch in enumerate(train_data_loader, start=1):\n",
    "\n",
    "            input_ids, token_type_ids, labels = batch\n",
    "            probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "            loss = criterion(probs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            correct = metric.compute(probs, labels)\n",
    "            metric.update(correct)\n",
    "            acc = metric.accumulate()\n",
    "\n",
    "            global_step += 1\n",
    "            \n",
    "            # 每间隔 100 step 输出训练指标\n",
    "            if global_step % 100 == 0:\n",
    "                print(\n",
    "                    \"global step %d, epoch: %d, batch: %d, loss: %.5f, accu: %.5f, speed: %.2f step/s\"\n",
    "                    % (global_step, epoch, step, loss, acc,\n",
    "                        10 / (time.time() - tic_train)))\n",
    "                tic_train = time.time()\n",
    "            \n",
    "\n",
    "            # 对抗训练\n",
    "            fgm.attack() # 在embedding上添加对抗扰动\n",
    "            loss_adv = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "            loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "            fgm.restore() # 恢复embedding参数\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if global_step % save_steps == 0 or global_step == last_step:\n",
    "                model_path=os.path.join(output_dir,\"model_classfication_%d.pdparams\" % global_step)\n",
    "                paddle.save(model.state_dict(),model_path)\n",
    "# 对抗训练\n",
    "#do_adversarial_train(model,train_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:30:18.909852Z",
     "iopub.status.busy": "2024-06-05T14:30:18.909273Z",
     "iopub.status.idle": "2024-06-05T14:30:19.115623Z",
     "shell.execute_reply": "2024-06-05T14:30:19.114775Z",
     "shell.execute_reply.started": "2024-06-05T14:30:18.909817Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'text': '共享一个额度，没啥必要，四个卡不要年费吗？你这种人头，银行最喜欢，广发是出了名的风控严，套现就给你封...', 'label': 0}, {'text': '炸了，就2000.浦发没那么好心，草', 'label': 0}, {'text': '挂了电话自己打过去分期提额可以少分一点的', 'label': 0}, {'text': '比如你首卡10k，二卡也10k，信报上显示邮政总共给你的授信额度是20k', 'label': 0}]\r\n"
     ]
    }
   ],
   "source": [
    "def read_text(data_path):\n",
    "    df=pd.read_csv(data_path)\n",
    "    for idx,row in df.iterrows():\n",
    "        words=row['text']\n",
    "        labels=0\n",
    "        yield {'text': words, 'label': labels}\n",
    "\n",
    "test_ds = load_dataset(read_text, data_path='data/test_public.csv',lazy=False)\n",
    "print(test_ds[:4])\n",
    "test_ds = test_ds.map(partial(convert_example, tokenizer=tokenizer))\n",
    "test_batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "        dataset=test_ds,\n",
    "        batch_sampler=test_batch_sampler,\n",
    "        return_list=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:30:29.158495Z",
     "iopub.status.busy": "2024-06-05T14:30:29.157894Z",
     "iopub.status.idle": "2024-06-05T14:31:01.591213Z",
     "shell.execute_reply": "2024-06-05T14:31:01.590239Z",
     "shell.execute_reply.started": "2024-06-05T14:30:29.158456Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def predict(model,test_data_loader):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    losses=[]\n",
    "    result=[]\n",
    "    for step, batch in enumerate(test_data_loader, start=1):\n",
    "        input_ids, token_type_ids, labels = batch\n",
    "        probs = model(input_ids=input_ids, token_type_ids=token_type_ids)\n",
    "        # print(probs)\n",
    "        out2 = paddle.argmax(probs, axis=1)\n",
    "        result.extend(out2.numpy().tolist())\n",
    "    return result\n",
    "\n",
    "static_dict=paddle.load('checkpoint/model_classfication_700.pdparams')\n",
    "model.load_dict(static_dict)\n",
    "result=predict(model,test_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-05T14:31:09.310977Z",
     "iopub.status.busy": "2024-06-05T14:31:09.310337Z",
     "iopub.status.idle": "2024-06-05T14:31:09.336031Z",
     "shell.execute_reply": "2024-06-05T14:31:09.335238Z",
     "shell.execute_reply.started": "2024-06-05T14:31:09.310932Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>BIO_anno</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>O O O O O O O O O B-BANK I-BANK O B-COMMENTS_A...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>O O O O O O O O O B-PRODUCT I-PRODUCT B-COMMEN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>O O O B-PRODUCT I-PRODUCT O O O O O O O O O O ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>O O O O O O B-BANK I-BANK O O O O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>O O O O O O O O O O O O O O O B-COMMENTS_N I-C...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>O O O O O O O O O O B-BANK I-BANK O</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                           BIO_anno  class\n",
       "0   0  O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...      1\n",
       "1   1  O O O O O O O O O B-BANK I-BANK O B-COMMENTS_A...      1\n",
       "2   2  O O O O O O O O O B-PRODUCT I-PRODUCT B-COMMEN...      1\n",
       "3   3  O O O B-PRODUCT I-PRODUCT O O O O O O O O O O ...      1\n",
       "4   4                  O O O O O O B-BANK I-BANK O O O O      1\n",
       "5   5  O O O O O O O O B-COMMENTS_ADJ I-COMMENTS_ADJ ...      1\n",
       "6   6  O O O O B-COMMENTS_N I-COMMENTS_N O O O O O O ...      1\n",
       "7   7  O O O O O O O O O O O O O O O B-COMMENTS_N I-C...      1\n",
       "8   8  O O O O O O O O O O O B-COMMENTS_N I-COMMENTS_...      1\n",
       "9   9                O O O O O O O O O O B-BANK I-BANK O      1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_data=[]\n",
    "for idx,(bio,cls) in enumerate(zip(bio_label,result)):\n",
    "    result_data.append([idx,bio,cls])\n",
    "\n",
    "submit=pd.DataFrame(result_data,columns=['id','BIO_anno','class'])\n",
    "submit.to_csv('submission_v1.csv',index=False)\n",
    "submit.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、模型优化思路\n",
    "\n",
    "1.数据增强： [中文数据增强工具](https://github.com/425776024/nlpcda/)、回译等\n",
    "\n",
    "2.尝试不同的预训练模型、调参优化等。\n",
    "\n",
    "3.5fodls交叉验证、多模型结果融合等\n",
    "\n",
    "4.能力较强的可以尝试下在数据上重新预训练或者魔改网络：尝试在命名实体识别任务中，在bert后添加lstm，crf；在情感分析任务中，尝试使用各种各样的预训练语言模型。\n",
    "\n",
    "\n",
    "关于paddlenlp：在具体使用时建议多看相关文档  [PaddleNLP文档](https://paddlenlp.readthedocs.io/zh/latest/get_started/quick_start.html)\n",
    "\n",
    "paddlenlp的github地址：https://github.com/PaddlePaddle/PaddleNLP  有问题的话可以在github上提issue。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、更多PaddleEdu信息内容\n",
    "### 1. PaddleEdu一站式深度学习在线百科[awesome-DeepLearning](https://github.com/paddlepaddle/awesome-DeepLearning)中还有其他的能力，大家可以敬请期待：\n",
    "\n",
    "* **深度学习入门课**\n",
    "\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/3373732039aa4a818dd8b1faefaa44a6b31497348c29446ca28dd218234814d1' width='700'></center>\n",
    "\n",
    "* **深度学习百问**\n",
    "\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/433d373002734631a1da0273b2fa9dfbb45279e97075429ca0530c5fa787b6d3' width='700'></center>\n",
    "\n",
    "* **特色课**\n",
    "\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/2b99d97c34ea4d168ce5a8e33fce28ef6c95fee5418f455cad15557952e8b0b2' width='700'></center>\n",
    "\n",
    "* **产业实践**\n",
    "\n",
    "<center><img src='https://ai-studio-static-online.cdn.bcebos.com/8a6ae927f7394737b6b2f13bf2d120c9970092ce59d44faa9ade500aa515ea19' width='700'></center>\n",
    "\n",
    "   PaddleEdu使用过程中有任何问题欢迎在[awesome-DeepLearning](https://github.com/paddlepaddle/awesome-DeepLearning)提issue，同时更多深度学习资料请参阅[飞桨深度学习平台](https://www.paddlepaddle.org.cn/?fr=paddleEdu_aistudio)。\n",
    "   \n",
    "   ### 记得点个[Star](https://github.com/PaddlePaddle/awesome-DeepLearning/stargazers)⭐收藏噢~~\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/94624c4ac79c4d9ba2a25b5cad6488f22a5baebacfcb414ea4980a37109e3ac0)\n",
    "\n",
    "### 2. 飞桨PaddleEdu技术交流群（QQ）\n",
    "\n",
    "目前QQ群已有2000+同学一起学习，欢迎扫码加入\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/17dd8cc4b14e40d091812d03780580672fe6ea6bb77e4f77889451cf1dcdc5ad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
